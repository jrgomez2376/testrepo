{"cells":[{"cell_type":"markdown","source":["<center><p float=\"center\">\n","  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n","  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n","</p></center>\n","\n","<h1><center><font size=10>Artificial Intelligence and Machine Learning</center></font></h1>\n","<h1><center>Introduction to Computer Vision - Plant Seedling</center></h1>"],"metadata":{"id":"qm7G4-DrbLmt"}},{"cell_type":"markdown","source":["<center><img src=\"https://cdn.pixabay.com/photo/2020/04/06/11/22/seedling-5009286_1280.jpg\" width=\"1300\" height=\"500\"></center>\n","\n","<b><h2><center> Plant Seedling Classification </center></h2></b>"],"metadata":{"id":"mX6oZaVobs3i"}},{"cell_type":"markdown","metadata":{"id":"v9VXT5unCaBd"},"source":["## Note: This is a sample solution for the project. Projects will NOT be graded on the basis of how well the submission matches this sample solution. Projects will be graded on the basis of the rubric only."]},{"cell_type":"markdown","source":["## Problem Statement"],"metadata":{"id":"zKED--jIcJhe"}},{"cell_type":"markdown","metadata":{"id":"WCxSmokWEKUJ"},"source":["### Context"]},{"cell_type":"markdown","source":["In recent times, the field of agriculture has been in urgent need of modernizing, since the amount of manual work people need to put in to check if plants are growing correctly is still highly extensive. Despite several advances in agricultural technology, people working in the agricultural industry still need to have the ability to sort and recognize different plants and weeds, which takes a lot of time and effort in the long term. The potential is ripe for this trillion-dollar industry to be greatly impacted by technological innovations that cut down on the requirement for manual labor, and this is where Artificial Intelligence can actually benefit the workers in this field, as **the time and energy required to identify plant seedlings will be greatly shortened by the use of AI and Deep Learning.** The ability to do so far more efficiently and even more effectively than experienced manual labor, could lead to better crop yields, the freeing up of human inolvement for higher-order agricultural decision making, and in the long term will result in more sustainable environmental practices in agriculture as well."],"metadata":{"id":"_jjqyY4ncO4u"}},{"cell_type":"markdown","source":["### Objective"],"metadata":{"id":"B4DxJnTbcSkC"}},{"cell_type":"markdown","source":["The aim of this project is to Build a Convolutional Neural Netowrk to classify plant seedlings into their respective categories."],"metadata":{"id":"l6T6Ps2mcVjD"}},{"cell_type":"markdown","source":["### Data Dictionary"],"metadata":{"id":"LHHIy7t5cZgE"}},{"cell_type":"markdown","source":["The Aarhus University Signal Processing group, in collaboration with the University of Southern Denmark, has recently released a dataset containing **images of unique plants belonging to 12 different species.**\n","\n","- The dataset can be download from Olympus.\n","- The data file names are:\n","    - images.npy\n","    - Label.csv\n","- Due to the large volume of data, the images were converted to the images.npy file and the labels are also put into Labels.csv, so that you can work on the data/project seamlessly without having to worry about the high data volume.\n","\n","- The goal of the project is to create a classifier capable of determining a plant's species from an image.\n","\n","**List of Species**\n","\n","- Black-grass\n","- Charlock\n","- Cleavers\n","- Common Chickweed\n","- Common Wheat\n","- Fat Hen\n","- Loose Silky-bent\n","- Maize\n","- Scentless Mayweed\n","- Shepherds Purse\n","- Small-flowered Cranesbill\n","- Sugar beet"],"metadata":{"id":"ix6qWAX6cb4X"}},{"cell_type":"markdown","metadata":{"id":"qqFzmTb0BKKW"},"source":["## Importing necessary libraries"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"lw8IuZwV-PAL","executionInfo":{"status":"ok","timestamp":1719329412615,"user_tz":300,"elapsed":214,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["import os\n","import numpy as np                                                                               # Importing numpy for Matrix Operations\n","import pandas as pd                                                                              # Importing pandas to read CSV files\n","import matplotlib.pyplot as plt                                                                  # Importting matplotlib for Plotting and visualizing images\n","import math                                                                                      # Importing math module to perform mathematical operations\n","import cv2                                                                                       # Importing openCV for image processing\n","import seaborn as sns                                                                            # Importing seaborn to plot graphs\n","\n","\n","# Tensorflow modules\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator                              # Importing the ImageDataGenerator for data augmentation\n","from tensorflow.keras.models import Sequential                                                   # Importing the sequential module to define a sequential model\n","from tensorflow.keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D,BatchNormalization # Defining all the layers to build our CNN Model\n","from tensorflow.keras.optimizers import Adam,SGD                                                 # Importing the optimizers which can be used in our model\n","from sklearn import preprocessing                                                                # Importing the preprocessing module to preprocess the data\n","from sklearn.model_selection import train_test_split                                             # Importing train_test_split function to split the data into train and test\n","from sklearn.metrics import confusion_matrix                                                     # Importing confusion_matrix to plot the confusion matrix\n","\n","# Display images using OpenCV\n","#from google.colab.patches import cv2_imshow                                                      # Importing cv2_imshow from google.patches to display images\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"W593kHe2Bfgp"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hToRNfavI2Ks","outputId":"61756a1f-6eff-4a61-92fe-4b29d33dfcdf","executionInfo":{"status":"ok","timestamp":1719329323207,"user_tz":300,"elapsed":114937,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google drive to access the dataset (monkeys_dataset.zip)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"c2q2QUVZtpFb","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1719329426928,"user_tz":300,"elapsed":224,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}},"outputId":"de341448-4a5f-470f-da06-1dab0767cb28"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'images.npy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-4a592cf4b4aa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the image file of dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the labels file of dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images.npy'"]}],"source":["# Load the image file of dataset\n","images = np.load('images.npy')\n","\n","# Load the labels file of dataset\n","labels = pd.read_csv('labels.csv')"]},{"cell_type":"markdown","metadata":{"id":"uE84hQU7CSZa"},"source":["## Data Overview"]},{"cell_type":"markdown","source":["### Understand the shape of the dataset"],"metadata":{"id":"R3rEAjqpc0Wb"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"2X1I66hDfg9j","outputId":"3502c7c2-fd95-41b2-d6d3-d3eeaf50275f","executionInfo":{"status":"error","timestamp":1719329352015,"user_tz":300,"elapsed":12,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'images' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-004278b726d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"]}],"source":["print(images.shape)\n","print(labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"FMA3e7ujCj1o"},"source":["There are 4750 RGB  images of shape 128 x 128 X 3 each. As mentioned, each image is an RGB image having 3 channels"]},{"cell_type":"markdown","metadata":{"id":"EYv5uX-MC9KC"},"source":["## Exploratory Data Analysis"]},{"cell_type":"markdown","source":["### Plotting random images from each of the class"],"metadata":{"id":"2vRE5H_MdNuH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxTvixIeBVIq","executionInfo":{"status":"aborted","timestamp":1719329323454,"user_tz":300,"elapsed":129847,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["def plot_images(images,labels):\n","  num_classes=10                                                                  # Number of Classes\n","  categories=np.unique(labels)\n","  keys=dict(labels['Label'])                                                      # Obtaing the unique classes from y_train\n","  rows = 3                                                                        # Defining number of rows=3\n","  cols = 4                                                                        # Defining number of columns=4\n","  fig = plt.figure(figsize=(10, 8))                                               # Defining the figure size to 10x8\n","  for i in range(cols):\n","      for j in range(rows):\n","          random_index = np.random.randint(0, len(labels))                        # Generating random indices from the data and plotting the images\n","          ax = fig.add_subplot(rows, cols, i * rows + j + 1)                      # Adding subplots with 3 rows and 4 columns\n","          ax.imshow(images[random_index, :])                                      # Plotting the image\n","          ax.set_title(keys[random_index])\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FclzjwV9hhb","executionInfo":{"status":"aborted","timestamp":1719329323454,"user_tz":300,"elapsed":129843,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["plot_images(images,labels)"]},{"cell_type":"markdown","metadata":{"id":"tY8e2flvDup8"},"source":["### Checking the distribution of the target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZTqGtN8D65D","executionInfo":{"status":"aborted","timestamp":1719329323806,"user_tz":300,"elapsed":39,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["sns.countplot(x=labels['Label'])\n","plt.xticks(rotation='vertical')"]},{"cell_type":"markdown","metadata":{"id":"8MIcAf2sWKbl"},"source":["- As you can observe from the above plot, the dataset is imbalanced.\n","- So we can try to use data augmentation techniques or use appropriate evaluation methods like confusion matrix, precision and recall to evaluate the model which is trained on imbalanced data."]},{"cell_type":"markdown","source":["## Data Pre-Processing"],"metadata":{"id":"sUAjhLr4dmca"}},{"cell_type":"markdown","metadata":{"id":"Yylo78cvIteK"},"source":["### Converting the BGR images to RGB images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tcc3Qjc0Iwkm","executionInfo":{"status":"aborted","timestamp":1719329323806,"user_tz":300,"elapsed":37,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Converting the images from BGR to RGB using cvtColor function of OpenCV\n","for i in range(len(images)):\n","  images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)"]},{"cell_type":"markdown","source":["### Resizing images"],"metadata":{"id":"VmJe42CJeDOx"}},{"cell_type":"markdown","metadata":{"id":"pESDU0AEMOFk"},"source":["As the size of the images is large, it may be computationally expensive to train on these larger images; therefore, it is preferable to reduce the image size from 128 to 64."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxhwEp-5MPqv","executionInfo":{"status":"aborted","timestamp":1719329323807,"user_tz":300,"elapsed":37,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["images_decreased=[]\n","height = 64\n","width = 64\n","dimensions = (width, height)\n","for i in range(len(images)):\n","  images_decreased.append( cv2.resize(images[i], dimensions, interpolation=cv2.INTER_LINEAR))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuEqXWMUMV_h","executionInfo":{"status":"aborted","timestamp":1719329323807,"user_tz":300,"elapsed":36,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["plt.imshow(images_decreased[8])"]},{"cell_type":"markdown","source":["### Data Preparation for Modeling"],"metadata":{"id":"T_EVdhKzeMCG"}},{"cell_type":"markdown","metadata":{"id":"NQV0unTvM7XM"},"source":["**Splitting the dataset**\n","\n","- As we have less images in our dataset, we will only use 10% of our data for testing, 10% of our data for validation and 80% of our data for training.\n","- We are using the train_test_split() function from scikit-learn. Here, we split the dataset into three parts, train,test and validation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1iq7Nm5M6QT","executionInfo":{"status":"aborted","timestamp":1719329323808,"user_tz":300,"elapsed":36,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_temp, X_test, y_temp, y_test = train_test_split(np.array(images_decreased),labels , test_size=0.1, random_state=42,stratify=labels)\n","X_train, X_val, y_train, y_val = train_test_split(X_temp,y_temp , test_size=0.1, random_state=42,stratify=y_temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8skQ0ZX-Nb7N","executionInfo":{"status":"aborted","timestamp":1719329323808,"user_tz":300,"elapsed":35,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["print(X_train.shape,y_train.shape)\n","print(X_val.shape,y_val.shape)\n","print(X_test.shape,y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"FAJ9B0wKNiY3"},"source":["### Encoding the target labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88OIAwNoEPfx","executionInfo":{"status":"aborted","timestamp":1719329323809,"user_tz":300,"elapsed":34,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Convert labels from names to one hot vectors.\n","# We have already used encoding methods like onehotencoder and labelencoder earlier so now we will be using a new encoding method called labelBinarizer.\n","# Labelbinarizer works similar to onehotencoder\n","\n","from sklearn.preprocessing import LabelBinarizer\n","enc = LabelBinarizer()\n","y_train_encoded = enc.fit_transform(y_train)\n","y_val_encoded=enc.transform(y_val)\n","y_test_encoded=enc.transform(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_owIjVzeEMol","executionInfo":{"status":"aborted","timestamp":1719329323809,"user_tz":300,"elapsed":33,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["y_train_encoded.shape,y_val_encoded.shape,y_test_encoded.shape"]},{"cell_type":"markdown","source":["### Data Normalization"],"metadata":{"id":"jdqjzH-per5n"}},{"cell_type":"markdown","metadata":{"id":"exJFCDSMNrEG"},"source":["Since the **image pixel values range from 0-255**, our method of normalization here will be **scaling** - we shall **divide all the pixel values by 255 to standardize the images to have values between 0-1.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVUuPJS9OB_U","executionInfo":{"status":"aborted","timestamp":1719329323809,"user_tz":300,"elapsed":32,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Normalizing the image pixels\n","X_train_normalized = X_train.astype('float32')/255.0\n","X_val_normalized = X_val.astype('float32')/255.0\n","X_test_normalized = X_test.astype('float32')/255.0"]},{"cell_type":"markdown","source":["## Model Building"],"metadata":{"id":"p97vhbn4fRKE"}},{"cell_type":"markdown","metadata":{"id":"_t86xTNQOMrw"},"source":["Let's create a CNN model sequentially, where we will be adding the layers one after another.\n","\n","First, we need to clear the previous model's history from the session even if a single model can run multiple times on the same data.\n","\n","In Keras, we need a special command to clear the model's history, otherwise the previous model history remains in the backend.\n","\n","Also, let's fix the seed again after clearing the backend.\n","\n","Let's **set the seed for random number generators in Numpy, the Random library in Python, and in TensorFlow** to be able to reproduce the same results every time we run the code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ze1uI-R1ObD8","executionInfo":{"status":"aborted","timestamp":1719329323810,"user_tz":300,"elapsed":31,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Clearing backend\n","from tensorflow.keras import backend\n","backend.clear_session()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIuK82kSOc0Z","executionInfo":{"status":"aborted","timestamp":1719329323810,"user_tz":300,"elapsed":30,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Fixing the seed for random number generators\n","import random\n","np.random.seed(42)\n","random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"koQqIIlPOiAc"},"source":["Now, let's build a **CNN Model** with the following 2 main parts - <br>\n","\n","1. **The Feature Extraction layers** which are comprised of convolutional and pooling layers.\n","2. **The Fully Connected classification layers** for prediction.<br><br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpFh02ziOcAy","executionInfo":{"status":"aborted","timestamp":1719329323810,"user_tz":300,"elapsed":30,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Intializing a sequential model\n","model1 = Sequential()\n","\n","# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n","# Input_shape denotes input image dimension of images\n","model1.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n","\n","# Adding max pooling to reduce the size of output of first conv layer\n","model1.add(MaxPooling2D((2, 2), padding = 'same'))\n","\n","# Creating two similar convolution and max-pooling layers\n","model1.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n","model1.add(MaxPooling2D((2, 2), padding = 'same'))\n","\n","model1.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n","model1.add(MaxPooling2D((2, 2), padding = 'same'))\n","\n","# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n","model1.add(Flatten())\n","\n","# Adding a fully connected dense layer with 100 neurons\n","model1.add(Dense(16, activation='relu'))\n","model1.add(Dropout(0.3))\n","# Adding the output layer with 10 neurons and activation functions as softmax since this is a multi-class classification problem\n","model1.add(Dense(12, activation='softmax'))\n","\n","# Using SGD Optimizer\n","# opt = SGD(learning_rate=0.01, momentum=0.9)\n","opt=Adam()\n","# Compile model\n","model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Generating the summary of the model\n","model1.summary()"]},{"cell_type":"markdown","metadata":{"id":"aeh0rJrYO7Cb"},"source":["<b> Fitting the model on the train data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSYgFjSfFZrS","executionInfo":{"status":"aborted","timestamp":1719329323811,"user_tz":300,"elapsed":30,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["history_1 = model1.fit(\n","            X_train_normalized, y_train_encoded,\n","            epochs=30,\n","            validation_data=(X_val_normalized,y_val_encoded),\n","            batch_size=32,\n","            verbose=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"QJner8d_WYBy"},"source":["**Model Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa8e-zpRWVyZ","executionInfo":{"status":"aborted","timestamp":1719329323811,"user_tz":300,"elapsed":28,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["plt.plot(history_1.history['accuracy'])\n","plt.plot(history_1.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Tk893TyNWfBm"},"source":["**Observations:**\n","\n","\n","*   We can see from the above plot that **the training accuracy of the  model was not good(63%) but the validation accuracy was good(71%).**\n","*   The shows that the model is not stable.\n","* Let's check the model performance on test set.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OkAhnIyqWqeM"},"source":["**Evaluating the model on test data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrdX25VOyhQ-","executionInfo":{"status":"aborted","timestamp":1719329323811,"user_tz":300,"elapsed":27,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["accuracy = model1.evaluate(X_test_normalized, y_test_encoded, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"z4fiOuFyxVS3"},"source":["**Plotting the Confusion Matrix**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MDuVQnmVc0h3"},"source":["\n","*   The Confusion matrix is also defined as an inbuilt function in the TensorFlow module, so we can use that for evaluating the classification model.\n","*   The Confusion matrix expects categorical data as input. However, y_test_encoded is an encoded value, whereas y_pred has probabilities. So,we must retrieve the categorical values from the encoded values.\n","*   We will use the `argmax()` function to obtain the maximum value over each category on both y_test_encoded and y_pred and obtain their respective classes.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lfy2ylNK7Zi_","executionInfo":{"status":"aborted","timestamp":1719329323812,"user_tz":300,"elapsed":27,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Here we would get the output as probablities for each category\n","y_pred=model1.predict(X_test_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbH18RpBdgLc","executionInfo":{"status":"aborted","timestamp":1719329323812,"user_tz":300,"elapsed":25,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Obtaining the categorical values from y_test_encoded and y_pred\n","y_pred_arg=np.argmax(y_pred,axis=1)\n","y_test_arg=np.argmax(y_test_encoded,axis=1)\n","\n","# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n","confusion_matrix = tf.math.confusion_matrix(y_test_arg,y_pred_arg)\n","f, ax = plt.subplots(figsize=(12, 12))\n","sns.heatmap(\n","    confusion_matrix,\n","    annot=True,\n","    linewidths=.4,\n","    fmt=\"d\",\n","    square=True,\n","    ax=ax\n",")\n","# Setting the labels to both the axes\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix');\n","ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n","ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NT92aWeGfuac"},"source":["<b> Observations</b><br>\n","- We observe that some of the classes are not predicted correctly.\n","- In comparison to the rest, we can see that classes common wheat,  black-grass, and shepherds-purse are not well classified.\n","- We can also observe that classification of black-grass is not being done, the black-grass is usually thin so it might even be a reason as the classifier is not able to classify this class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GskI2Ay0OkB4","executionInfo":{"status":"aborted","timestamp":1719329323812,"user_tz":300,"elapsed":24,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["from sklearn import metrics\n","#Accuracy as per the classification report\n","cr=metrics.classification_report(y_test_arg,y_pred_arg)\n","print(cr)"]},{"cell_type":"markdown","metadata":{"id":"e8zEzmgffxZi"},"source":["<b> Deleting the model and history variable to use the RAM efficiently"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0fnV8yNKmYr","executionInfo":{"status":"aborted","timestamp":1719329323812,"user_tz":300,"elapsed":23,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["del model1\n","del history_1"]},{"cell_type":"markdown","metadata":{"id":"kNKUalx8Jcoi"},"source":["## Model Performance Improvement"]},{"cell_type":"markdown","metadata":{"id":"s_oS4D_AXFqX"},"source":["**Reducing the Learning Rate:**\n","\n","**ReduceLRonPlateau()** is a function that will be used to decrease the learning rate by some factor, if the loss is not decreasing for some time. This may start decreasing the loss at a smaller learning rate. There is a possibility that the loss may still not decrease. This may lead to executing the learning rate reduction again in an attempt to achieve a lower loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlRncTSBTtj5","executionInfo":{"status":"aborted","timestamp":1719329323812,"user_tz":300,"elapsed":22,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["from keras.callbacks import ReduceLROnPlateau\n","\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n","                                            patience=3,\n","                                            verbose=1,\n","                                            factor=0.5,\n","                                            min_lr=0.00001)\n"]},{"cell_type":"markdown","metadata":{"id":"zU6vqL67bd5a"},"source":["### **Data Augmentation**\n","\n","\n","Remember, **data augmentation should not be used in the validation/test data set**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcyB5b4QTeLz","executionInfo":{"status":"aborted","timestamp":1719329323813,"user_tz":300,"elapsed":22,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Clearing backend\n","from tensorflow.keras import backend\n","backend.clear_session()\n","\n","# Fixing the seed for random number generators\n","import random\n","np.random.seed(42)\n","random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PC2Vb1Vph3Cq","executionInfo":{"status":"aborted","timestamp":1719329323813,"user_tz":300,"elapsed":21,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","                              rotation_range=20,\n","                              fill_mode='nearest'\n","                              )\n","# test_datagen  = ImageDataGenerator(rescale = 1.0/255.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-oolHyJh6Wp","executionInfo":{"status":"aborted","timestamp":1719329323813,"user_tz":300,"elapsed":20,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Intializing a sequential model\n","model2 = Sequential()\n","\n","# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n","# Input_shape denotes input image dimension images\n","model2.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n","\n","# Adding max pooling to reduce the size of output of first conv layer\n","model2.add(MaxPooling2D((2, 2), padding = 'same'))\n","# model.add(BatchNormalization())\n","model2.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n","model2.add(MaxPooling2D((2, 2), padding = 'same'))\n","model2.add(BatchNormalization())\n","# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n","model2.add(Flatten())\n","\n","# Adding a fully connected dense layer with 100 neurons\n","model2.add(Dense(16, activation='relu'))\n","model2.add(Dropout(0.3))\n","# Adding the output layer with 10 neurons and activation functions as softmax since this is a multi-class classification problem\n","model2.add(Dense(12, activation='softmax'))\n","\n","# Using SGD Optimizer\n","# opt = SGD(learning_rate=0.01, momentum=0.9)\n","opt=Adam()\n","# Compile model\n","model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Generating the summary of the model\n","model2.summary()"]},{"cell_type":"markdown","source":["<b> Fitting the model on the train data"],"metadata":{"id":"JHCL3erxgH7Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChUJ8eeKYbNj","executionInfo":{"status":"aborted","timestamp":1719329323813,"user_tz":300,"elapsed":19,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Epochs\n","epochs = 30\n","# Batch size\n","batch_size = 64\n","\n","history = model2.fit(train_datagen.flow(X_train_normalized,y_train_encoded,\n","                                       batch_size=batch_size,\n","                                       shuffle=False),\n","                                       epochs=epochs,\n","                                       steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n","                                       validation_data=(X_val_normalized,y_val_encoded),\n","                                       verbose=1,callbacks=[learning_rate_reduction])"]},{"cell_type":"markdown","source":["**Model Evaluation**"],"metadata":{"id":"QSCgTO5jo1am"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHiqcKS9zsK1","executionInfo":{"status":"aborted","timestamp":1719329323814,"user_tz":300,"elapsed":20,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","source":["**Evaluate the model on test data**"],"metadata":{"id":"ZnvaSsDBo6JV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQQWGjwhzuQR","executionInfo":{"status":"aborted","timestamp":1719329323814,"user_tz":300,"elapsed":19,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["accuracy = model2.evaluate(X_test_normalized, y_test_encoded, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"ucl9zptMzzyO"},"source":["* We can observe that our accuracy has improved compared to our previous model.\n","* The model is giving a generalized performance."]},{"cell_type":"markdown","source":["**Plotting the Confusion Matrix**"],"metadata":{"id":"MwZUZEFeo-gG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVGeFvZYYP2N","executionInfo":{"status":"aborted","timestamp":1719329323814,"user_tz":300,"elapsed":19,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Here we would get the output as probablities for each category\n","y_pred=model2.predict(X_test_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JX5vAeYCz7qF","executionInfo":{"status":"aborted","timestamp":1719329323814,"user_tz":300,"elapsed":18,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Obtaining the categorical values from y_test_encoded and y_pred\n","y_pred_arg=np.argmax(y_pred,axis=1)\n","y_test_arg=np.argmax(y_test_encoded,axis=1)\n","\n","# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n","confusion_matrix = tf.math.confusion_matrix(y_test_arg,y_pred_arg)\n","f, ax = plt.subplots(figsize=(12, 12))\n","sns.heatmap(\n","    confusion_matrix,\n","    annot=True,\n","    linewidths=.4,\n","    fmt=\"d\",\n","    square=True,\n","    ax=ax\n",")\n","# Setting the labels to both the axes\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix');\n","ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n","ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FuFnTU_7YTD0"},"source":["**Observation:**\n","- The confusion matrix appears to be improving as well, however there is still some confusion with the black-grass, common-wheat and shepherds purse species.\n","- The black-grass and common wheat class is the most confused class among all.\n","- We can observe that this model has outperformed our previous model and this has given improved performance than our previous model."]},{"cell_type":"markdown","source":["**Plotting Classification Report**"],"metadata":{"id":"hoBZ8lUzpDtm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFO0ol_BOkB6","executionInfo":{"status":"aborted","timestamp":1719329323815,"user_tz":300,"elapsed":19,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["#Accuracy as per the classification report\n","cr=metrics.classification_report(y_test_arg,y_pred_arg)\n","print(cr)"]},{"cell_type":"markdown","metadata":{"id":"dvDkLMO7YIdY"},"source":["## Visualizing the prediction with the best model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IMupJqCDlWR","executionInfo":{"status":"aborted","timestamp":1719329323815,"user_tz":300,"elapsed":19,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Visualizing the predicted and correct label of images from test data\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[2])\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(model2.predict((X_test_normalized[2].reshape(1,64,64,3)))))   # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[2])                                               # using inverse_transform() to get the output label from the output vector\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[33])\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(model2.predict((X_test_normalized[33].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[33])                                              # using inverse_transform() to get the output label from the output vector\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[59],)\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(model2.predict((X_test_normalized[59].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[59])                                              # using inverse_transform() to get the output label from the output vector\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[36])\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(model2.predict((X_test_normalized[36].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[36])                                              # using inverse_transform() to get the output label from the output vector"]},{"cell_type":"markdown","metadata":{"id":"CmWz9WYp4M-i"},"source":["**Observations**\n","- We observe that all the above images were classified correctly.\n","- This shows that data augmentation has helped in creating a generalized model."]},{"cell_type":"markdown","metadata":{"id":"Eg2x8AyJ4oPR"},"source":["## Actionable Insights and Business Recommendations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJYwwuY61MFk","executionInfo":{"status":"aborted","timestamp":1719329323815,"user_tz":300,"elapsed":19,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["pd.DataFrame({'Models':['Base CNN Model','CNN Model with Data Augmentation'],'Train Accuracy':['44%','75%'],'Validation Accuracy':['61%','72%'],'Test Accuracy':['60%','70%']})"]},{"cell_type":"markdown","metadata":{"id":"9A3gWIwn1FbB"},"source":["- We can observe that our second model is the best model as it predicted the majority of the classes correctly and also provided better accuracy.\n","- The performance of the model is generalized as the training accuracy is 75% and the test accuracy is 70%.\n","- Data Augmentation has also helped in improving the overall model performance.\n","- The model seems to be struggling in predicting the 'Black-grass' plant seeds as it has the most misclassifications."]},{"cell_type":"markdown","metadata":{"id":"ZyrgrSq8YB1d"},"source":["# **Additional** -  Transfer Learning using VGG16"]},{"cell_type":"markdown","metadata":{"id":"cU0StUb70PGn"},"source":["- We will be using the idea of **Transfer Learning**. We will be loading a pre-built architecture - **VGG16**, which was trained on the ImageNet dataset and is the runner-up in the ImageNet competition in 2014.\n","\n","- For training VGG16, we will directly use the convolutional and pooling layers and freeze their weights i.e. no training will be done on them. For classification, we will replace the existing fully-connected layers with FC layers created specifically for our problem.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pxh29Re323_h","executionInfo":{"status":"aborted","timestamp":1719329323815,"user_tz":300,"elapsed":18,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Clearing backend\n","from tensorflow.keras import backend\n","backend.clear_session()\n","\n","# Fixing the seed for random number generators\n","import random\n","np.random.seed(42)\n","random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0pbpH-mzzaQ","executionInfo":{"status":"aborted","timestamp":1719329323818,"user_tz":300,"elapsed":21,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","\n","vgg_model = VGG16(weights='imagenet', include_top = False, input_shape = (64,64,3))   # Importing the VGG16 Model with pretrained ImageNet weights\n","vgg_model.summary()                                                                   # Summary of the VGG16 Model without the dense layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNKcAv-50XHI","executionInfo":{"status":"aborted","timestamp":1719329323818,"user_tz":300,"elapsed":130123,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Making all the layers of the VGG model non-trainable. i.e. freezing them\n","for layer in vgg_model.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMxnIwjo0aF6","executionInfo":{"status":"aborted","timestamp":1719329323819,"user_tz":300,"elapsed":130122,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["model3 = Sequential()\n","\n","# Adding the convolutional part of the VGG16 model from above\n","model3.add(vgg_model)\n","\n","# Flattening the output of the VGG16 model because it is from a convolutional layer\n","model3.add(Flatten())\n","\n","# Adding a dense layer\n","model3.add(Dense(32, activation='relu'))\n","# Adding droput layer with 0.2 dropout rate\n","model3.add(Dropout(0.2))\n","model3.add(Dense(16, activation='relu'))\n","# Adding the output layer with 12 neurons\n","model3.add(Dense(12, activation='softmax'))\n","opt=Adam()\n","# Compile model\n","model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Generating the summary of the model\n","model3.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxauDNi-0ms7","executionInfo":{"status":"aborted","timestamp":1719329323819,"user_tz":300,"elapsed":130119,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Epochs\n","epochs = 30\n","# Batch size\n","batch_size = 64\n","# Fit the model with augmented data\n","history_vgg16 = model3.fit(train_datagen.flow(X_train_normalized,y_train_encoded,\n","                                       batch_size=batch_size,\n","                                       seed=42,\n","                                       shuffle=False),\n","                    epochs=epochs,\n","                    steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n","                    validation_data=(X_val_normalized,y_val_encoded),\n","                    verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"Sql7zEWg6S1A"},"source":["### Plotting the Epoch vs Accuracy curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvKabULe0q2B","executionInfo":{"status":"aborted","timestamp":1719329323820,"user_tz":300,"elapsed":130117,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["plt.plot(history_vgg16.history['accuracy'])\n","plt.plot(history_vgg16.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ht9tx3IO7_AI"},"source":["### Observations\n","- We can observe that the VGG16 model is generalized with good accuracy but it is not better than our previous model(model-2)."]},{"cell_type":"markdown","metadata":{"id":"EGIZGFEo8fYR"},"source":["### Accuracy of the VGG16 Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30u8vA6K0tZS","executionInfo":{"status":"aborted","timestamp":1719329323820,"user_tz":300,"elapsed":130113,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Getting the accuracy of the model\n","accuracy = model3.evaluate(X_test_normalized, y_test_encoded, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5i9eBET0wtY","executionInfo":{"status":"aborted","timestamp":1719329323821,"user_tz":300,"elapsed":130113,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Here we would get the output as probablities for each category\n","y_pred=model3.predict(X_test_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7DwEYuO01pD","executionInfo":{"status":"aborted","timestamp":1719329323821,"user_tz":300,"elapsed":130111,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["# Obtaining the categorical values from y_test_encoded and y_pred\n","y_pred_arg=np.argmax(y_pred,axis=1)\n","y_test_arg=np.argmax(y_test_encoded,axis=1)\n","\n","# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n","confusion_matrix = tf.math.confusion_matrix(y_test_arg,y_pred_arg)\n","f, ax = plt.subplots(figsize=(12, 12))\n","sns.heatmap(\n","    confusion_matrix,\n","    annot=True,\n","    linewidths=.4,\n","    fmt=\"d\",\n","    square=True,\n","    ax=ax\n",")\n","# Setting the labels to both the axes\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix');\n","ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n","ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NMoxsZsJ082b"},"source":["### Observations\n","- According to the confusion matrix and accuracy curve, the VGG16 model does not outperform Model-2. This could be due to the data we're using; since we're using plant seedlings data, there's a chance that these images aren't in the ImageNet dataset, whose weights have been used to build our CNN model.\n","- Although VGGnet did not outperform Model-2, it is evident that simply employing the transfer learning model without any tuning performed can produce a better outcome than any ordinary CNN.\n","- Thus we can say that Model-2 is our best model and we can use it model to classify plant seedlings.\n"]},{"cell_type":"markdown","metadata":{"id":"MRH0REt937AX"},"source":["### Comparing the model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lobONo44BaO","executionInfo":{"status":"aborted","timestamp":1719329323821,"user_tz":300,"elapsed":130108,"user":{"displayName":"J. Ruben Gomez","userId":"09273660279341941513"}}},"outputs":[],"source":["pd.DataFrame({'Models':['Base CNN Model','CNN Model with Data Augmentation','Transfer Learning Model'],'Train Accuracy':['44%','75%','63%'],'Validation Accuracy':['61%','72%','60%'],'Test Accuracy':['60%','70%','60%']})"]},{"cell_type":"markdown","metadata":{"id":"LV9HVTqW6Blr"},"source":["* Transfer learning model provided a generalized performance as compared to the base CNN model.\n","* Model-2 with data augmentation outperformed the other two models."]},{"cell_type":"markdown","metadata":{"id":"Rr11Gd1Q1TgQ"},"source":["# **Happy Learning!**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["zKED--jIcJhe","WCxSmokWEKUJ","LHHIy7t5cZgE","qqFzmTb0BKKW","W593kHe2Bfgp","uE84hQU7CSZa","R3rEAjqpc0Wb","EYv5uX-MC9KC","2vRE5H_MdNuH","tY8e2flvDup8","sUAjhLr4dmca","Yylo78cvIteK","VmJe42CJeDOx","T_EVdhKzeMCG","FAJ9B0wKNiY3","jdqjzH-per5n","p97vhbn4fRKE","kNKUalx8Jcoi","zU6vqL67bd5a","dvDkLMO7YIdY","Eg2x8AyJ4oPR","ZyrgrSq8YB1d"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}